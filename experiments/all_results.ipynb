{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d9397c-a7ac-4d91-8a80-472396f84f52",
   "metadata": {},
   "source": [
    "# Visualization of results\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf91ec77-8a1c-43ad-9871-21235fc887b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import inspect\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac9d5b-1802-494e-9030-6c2a29e3070d",
   "metadata": {},
   "source": [
    "## Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30535301-11ba-4061-b30c-14d8320e90ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method to compute the geometric mean\n",
    "def geometric_mean(x):\n",
    "    return np.exp(np.log(x).mean())\n",
    "\n",
    "# Define a method to compute the average\n",
    "def average(x):\n",
    "    return x.mean()\n",
    "\n",
    "def plot_distribution_by_algorithm(df, algorithms_list, fields, filter_condition_1=None, filter_condition_2=None, fontsize=13, num_bins=30):\n",
    "    # Create plots for each algorithm\n",
    "    for algorithm in algorithms_list:\n",
    "        subset = df[df['algorithm'] == algorithm]\n",
    "\n",
    "        # Apply the filter condition (if any) to the subset for the second histogram\n",
    "        if filter_condition_1:\n",
    "            filtered_subset_1 = subset[filter_condition_1(subset)] \n",
    "        # Apply the filter condition (if any) to the subset for the third histogram\n",
    "        if filter_condition_1 and filter_condition_2:\n",
    "            filtered_subset_2 = subset[filter_condition_2(subset)] \n",
    "\n",
    "        # Create a figure with subplots side by side\n",
    "        fig, axes = plt.subplots(1, len(fields), figsize=(16, 4))\n",
    "\n",
    "        for i, field in enumerate(fields):\n",
    "            # Create logarithmic bins\n",
    "            log_bins = np.logspace(np.log10(subset[field].min()/1.1), np.log10(subset[field].max() * 1.1), num=num_bins)\n",
    "            \n",
    "            # Plot first histogram (original subset)\n",
    "            axes[i].hist(subset[field], bins=log_bins, color='lightblue', alpha=0.7, edgecolor='darkblue', label='All Runs')\n",
    "\n",
    "            # Plot second histogram (filtered subset)\n",
    "            if filter_condition_1:\n",
    "                axes[i].hist(filtered_subset_1[field], bins=log_bins, color='lightgreen', alpha=0.7, edgecolor='darkgreen', label='Filtered Runs')\n",
    "\n",
    "                # Plot third histogram (filtered subset)\n",
    "            if filter_condition_1 and filter_condition_2:\n",
    "                axes[i].hist(filtered_subset_2[field], bins=log_bins, color='lightpink', alpha=0.7, edgecolor='darkred', label='Second Filtered Runs')\n",
    "\n",
    "            # Set plot properties\n",
    "            axes[i].set_xscale('log')  # Set x-axis to logarithmic scale\n",
    "            axes[i].set_title(f'{field} distribution for {algorithm}', fontsize=fontsize)\n",
    "            axes[i].set_xlabel(f'{field} (log scale)', fontsize=fontsize)\n",
    "            axes[i].set_ylabel('Number of Runs', fontsize=fontsize)\n",
    "            axes[i].grid(True, linestyle=':')\n",
    "\n",
    "            # Add legend to differentiate between the histograms\n",
    "            axes[i].legend()\n",
    "\n",
    "        num_total_runs = len(subset)\n",
    "\n",
    "        relative_stats = {\n",
    "            'num_runs_less_1_sec': (subset['time'] <= 1).sum(),\n",
    "            'num_runs_more_1_min': (subset['time'] >= 60).sum(),\n",
    "            #'num_runs_timeout': (subset['time'] >= 2 * 60 * 60).sum(),\n",
    "            'num_runs_exact': (subset['is_exact'] == 1).sum(),\n",
    "            'num_runs_!naive': ((subset['mincut'] < subset['naive_mincut']) & subset['mincut'].notnull()).sum(),\n",
    "            'num_runs_!naive_&_!0': ((subset['mincut'] < subset['naive_mincut']) & subset['mincut'].notnull() & subset['mincut'] != 0).sum(),\n",
    "            'num_runs_aborted': (subset['mincut'].isnull()).sum()\n",
    "        }\n",
    "        \n",
    "        print(f\"#################### {algorithm} ####################\")\n",
    "        print(f\"num_total_runs: \\t\\t {num_total_runs}\")\n",
    "        for field in ['time', 'memory']:\n",
    "            print(f\"avg_total_{field}: \\t\\t {average(subset[field]):.3f}\")\n",
    "            print(f\"geo_mean_total_{field}: \\t\\t {geometric_mean(subset[field]):.3f}\")\n",
    "        for name, stat in relative_stats.items():\n",
    "            print(f\"{name}: \\t\\t {stat} \\t ({stat/num_total_runs * 100:.3f}%)\")\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(f'./{algorithm}/dist_{algorithm}.pdf')\n",
    "        plt.show()\n",
    "\n",
    "def plot_reduction_by_algorithm(algorithms_list, num_rounds=None, hypergraph_list=[], num_suffix_param_columns=4, fontsize=13):\n",
    "    # Create plots for each algorithm\n",
    "    for algorithm in algorithms_list:    \n",
    "        # Define the path of the \"all_reductions.csv\" file\n",
    "        all_reductions_path = f'./{algorithm}/all_reductions.csv'\n",
    "        # Store the number of runs where no reduction rule was applied (i.e. naive mincut is already 0)\n",
    "        num_runs_instant_exit = 0\n",
    "        # Get the number of reductions in the preprocessing\n",
    "        num_reductions_preprocessing = 1\n",
    "        # Get the number of reductions per round\n",
    "        num_reductions_per_round = 4 if \"_IT0\" in algorithm else 5\n",
    "\n",
    "        if os.path.exists(all_reductions_path):\n",
    "            with open(all_reductions_path, 'r') as file:\n",
    "                reader = csv.reader(file, delimiter=';')\n",
    "                # Store all rows of the CSV file\n",
    "                all_rows = [row for row in reader]\n",
    "                # Find the maximum number of columns in any row\n",
    "                max_num_columns = max(len(row) for row in all_rows)\n",
    "                # Cut off some reductions if a limit is set\n",
    "                if num_rounds is not None:\n",
    "                    max_num_columns = min(max_num_columns, 2 + 3 * (num_reductions_per_round + num_rounds * num_reductions_per_round) + num_suffix_param_columns)\n",
    "                # The rows can have different number of columnns\n",
    "                # Adjust each row so that the last three columns are aligned\n",
    "                aligned_rows = []\n",
    "                for row in all_rows:\n",
    "                    # Ingore those rows where no reduction rule was applied (i.e. naive mincut is already 0)\n",
    "                    if row[2] == '':\n",
    "                        num_runs_instant_exit += 1\n",
    "                        continue\n",
    "                    # Align the row if necessary\n",
    "                    if max_num_columns == len(row):\n",
    "                        aligned_rows.append(row)\n",
    "                    elif max_num_columns < len(row):\n",
    "                        aligned_rows.append(row[:(max_num_columns-num_suffix_param_columns)] + row[-num_suffix_param_columns:])\n",
    "                    else:\n",
    "                        aligned_rows.append(row[:-num_suffix_param_columns] + [pd.NA, pd.NA, pd.NA] * ((max_num_columns - len(row))//3 - 1) + [row[-num_suffix_param_columns-3], row[-num_suffix_param_columns-2], pd.NA] + row[-num_suffix_param_columns:])\n",
    "            all_reductions_df = pd.DataFrame(aligned_rows)\n",
    "            if len(hypergraph_list) > 0:\n",
    "                all_reductions_df = all_reductions_df[all_reductions_df.iloc[:, -3].isin(hypergraph_list)]\n",
    "        else:\n",
    "            print(f\"File does not exist: {all_reductions_path}\")\n",
    "            continue\n",
    "                            \n",
    "        # For the base_solver_time, replace empty fields with NaN and convert to a nullable float (= Float64)\n",
    "        all_reductions_df[all_reductions_df.columns[-num_suffix_param_columns]] = all_reductions_df[all_reductions_df.columns[-num_suffix_param_columns]].replace('', pd.NA).astype('Float64')\n",
    "\n",
    "        # Copy the dataframe to compute the relative reductions to the initial size (in percentages)\n",
    "        relative_all_reductions_df = all_reductions_df.copy()\n",
    "    \n",
    "        # Create three figures (reduction of edges, reduction of nodes, time per reduction)\n",
    "        fig = plt.figure(figsize=(16, 10))\n",
    "        gs = gridspec.GridSpec(3, 6, height_ratios=[1, 0.5, 0.75])\n",
    "    \n",
    "        # Perform the following operations for the edges (i = 0) and for the vertices (i = 1)\n",
    "        for i, metric in enumerate(['edges', 'nodes']):\n",
    "            # Get the columns containing the reductions (including the initial number)\n",
    "            columns = all_reductions_df.columns[i:i+1].tolist() + all_reductions_df.columns[2+i:-num_suffix_param_columns:3].tolist()\n",
    "\n",
    "            # Divide each column by the initial size to get the relative size (in percentages)\n",
    "            for col in columns: \n",
    "                all_reductions_df[col] = all_reductions_df[col].astype('Int64')\n",
    "                relative_all_reductions_df[col] = (all_reductions_df[col] / all_reductions_df[columns[0]]) * 100    \n",
    "            \n",
    "            # Plot the relative reductions of all runs of the current algorithm\n",
    "            positions = [gs[0, :3], gs[0, 3:]]\n",
    "            ax = fig.add_subplot(positions[i])\n",
    "            for j in range(len(relative_all_reductions_df)):\n",
    "                ax.plot(range(len(columns)), relative_all_reductions_df[columns].iloc[j], alpha=0.4)\n",
    "            \n",
    "            ax.set_title(f'% of remaining {metric} per reduction step for {algorithm}', fontsize=fontsize)\n",
    "            ax.set_xlabel('reduction step', fontsize=fontsize)\n",
    "            ax.set_ylabel(f'% of remaining {metric}', fontsize=fontsize)\n",
    "            ax.set_ylim([-10, 110])\n",
    "            ax.set_xlim([0, len(columns) - 1])\n",
    "            ax.grid(True, linestyle=':')\n",
    "\n",
    "        # Get the columns containing the reduction times\n",
    "        columns = all_reductions_df.columns[4:-num_suffix_param_columns:3].tolist()\n",
    "        # Plot the time of all runs of the current algorithm\n",
    "        ax3 = fig.add_subplot(gs[1, :])\n",
    "        for col in columns: \n",
    "            all_reductions_df[col] = all_reductions_df[col].astype('Float64')\n",
    "        for j in range(len(all_reductions_df)):\n",
    "            ax3.plot(range(1, len(columns) + 1), all_reductions_df[columns].iloc[j], alpha=0.4)  \n",
    "        ax3.set_title(f'time per reduction step for {algorithm}', fontsize=fontsize)\n",
    "        ax3.set_xlabel('reduction step', fontsize=fontsize)\n",
    "        ax3.set_ylabel(f'time', fontsize=fontsize)\n",
    "        ax3.set_xlim([1, len(columns)])\n",
    "        ax3.set_yscale('log')\n",
    "        ax3.grid(True, linestyle=':')\n",
    "\n",
    "        # Compute the effectiveness of each reduction rule for all metrics\n",
    "        # NB: For the metric time, the effectiveness is simply the time used\n",
    "        for i, metric in enumerate(['edges', 'nodes', 'time']):\n",
    "            effectiveness_list = []\n",
    "            for j in range(num_reductions_per_round):\n",
    "                # Get the base indices of each occurence of the given reduction rule \n",
    "                # NB1: This is always the #edges after the reduction rule\n",
    "                base_indices = np.array(range(2 + 3 * (num_reductions_preprocessing + j), all_reductions_df.shape[1] - num_suffix_param_columns, 3 * num_reductions_per_round))\n",
    "                # Compute the effectiveness (i.e. size after reduction rule relative to before when metric is edges or nodes, otherwise simply the time)\n",
    "                # NB1: Now we can drop the first column\n",
    "                # NB2: We need to make sure that we do not divide by zero\n",
    "                if metric == 'time':\n",
    "                    effectiveness_df = all_reductions_df.iloc[:, base_indices + i].astype('Float64')\n",
    "                else:\n",
    "                    effectiveness_df = 100 * (1 - all_reductions_df.iloc[:, base_indices + i].astype('Int64').div(all_reductions_df.iloc[:, base_indices + i - 3].astype('Int64').replace(0, pd.NA).values))\n",
    "                # Since we previously aligned the rows, we need to make sure that we only keep the columns which do not act as a placeholder (i.e. time is not NaN)\n",
    "                validity_df = (all_reductions_df.iloc[:, base_indices + 2].notna())\n",
    "                validity_df.columns = effectiveness_df.columns                \n",
    "                # NB: It is important to use 'float' instead of 'Float64' because afterwards we use np.round which needs np.nan and not pd.NA\n",
    "                effectiveness_df = effectiveness_df.where(validity_df).astype('float')\n",
    "                effectiveness_list.append(np.round(average(effectiveness_df.stack()), 3))\n",
    "\n",
    "            # Plot the effectiveness of each reduction rule\n",
    "            positions = [gs[2, :2], gs[2, 2:4], gs[2, 4:]]\n",
    "            ax_eff = fig.add_subplot(positions[i])\n",
    "            effectiveness_labels = ['no_heavy_edges', 'no_heavy_overlaps', 'no_shiftable_2-edges', 'no_triangle_2-edges']\n",
    "            if not \"_IT0\" in algorithm:\n",
    "                effectiveness_labels.insert(0, 'label_propagation')\n",
    "            bars = ax_eff.bar(effectiveness_labels, effectiveness_list, color='lightblue', alpha=0.7, edgecolor='darkblue')\n",
    "            ax_eff.set_xlabel('reduction rule', fontsize=fontsize)\n",
    "\n",
    "            if metric == 'time':\n",
    "                ax_eff.set_title('average time usage (s) for each rule', fontsize=fontsize)\n",
    "                ax_eff.set_ylabel('average time usage (s)', fontsize=fontsize)\n",
    "            else:\n",
    "                ax_eff.set_title(f'% of average {metric} reduction for each rule', fontsize=fontsize)\n",
    "                ax_eff.set_ylabel(f'% of average {metric} reduction', fontsize=fontsize)\n",
    "                ax_eff.set_ylim([0, 100])\n",
    "                ax_eff.set_yticks(ticks=range(0, 101, 10))\n",
    "            ax_eff.grid(True, linestyle=':')\n",
    "            # Set font size and rotation of x tick labels\n",
    "            for label in ax_eff.get_xticklabels():\n",
    "                label.set_fontsize(10)\n",
    "                label.set_rotation(10)\n",
    "\n",
    "            # Add value labels on top of each bar\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                text = ax_eff.text(bar.get_x() + bar.get_width()/2, height, f'{height:.2f}', ha='center', va='center', color='white', fontsize=8, weight='bold')\n",
    "                text.set_bbox(dict(boxstyle='square,pad=0.2', facecolor='darkblue',edgecolor='darkblue'))\n",
    "\n",
    "        relative_stats = {\n",
    "            'num_runs_instant_exit': num_runs_instant_exit,\n",
    "            'num_runs_max_reduced': ((all_reductions_df.iloc[:, -num_suffix_param_columns-3] == 0) | (all_reductions_df.iloc[:, -num_suffix_param_columns-2] == 1)).sum(),\n",
    "            'num_runs_base_solver': (all_reductions_df.iloc[:, -num_suffix_param_columns].notna()).sum()\n",
    "        }\n",
    "\n",
    "        num_total_runs = len(all_reductions_df) + num_runs_instant_exit\n",
    "        print(f\"#################### {algorithm} ####################\")\n",
    "        print(f\"num_total_runs: \\t\\t {num_total_runs}\")\n",
    "        for name, stat in relative_stats.items():\n",
    "            print(f\"{name}: \\t\\t {stat} \\t ({stat/num_total_runs * 100:.3f}%)\")\n",
    "        print(f\"base_solver_avg_edges: \\t\\t {average(all_reductions_df[all_reductions_df.iloc[:, -num_suffix_param_columns].notna()].iloc[:, -num_suffix_param_columns-3]):.3f}\")\n",
    "        print(f\"base_solver_geo_mean_edges: \\t {geometric_mean(all_reductions_df[all_reductions_df.iloc[:, -num_suffix_param_columns].notna()].iloc[:, -num_suffix_param_columns-3]):.3f}\")\n",
    "        print(f\"base_solver_avg_nodes: \\t\\t {average(all_reductions_df[all_reductions_df.iloc[:, -num_suffix_param_columns].notna()].iloc[:, -num_suffix_param_columns-2]):.3f}\")\n",
    "        print(f\"base_solver_geo_mean_nodes: \\t {geometric_mean(all_reductions_df[all_reductions_df.iloc[:, -num_suffix_param_columns].notna()].iloc[:, -num_suffix_param_columns-2]):.3f}\")\n",
    "        print(f\"base_solver_avg_time: \\t\\t {average(all_reductions_df[all_reductions_df.iloc[:, -num_suffix_param_columns].notna()].iloc[:, -num_suffix_param_columns]):.3f}\")\n",
    "        print(f\"base_solver_geo_mean_time: \\t {geometric_mean(all_reductions_df[all_reductions_df.iloc[:, -num_suffix_param_columns].notna()].iloc[:, -num_suffix_param_columns]):.3f}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'./{algorithm}/reduction_{algorithm}.pdf')\n",
    "        plt.show()\n",
    "\n",
    "def plot_performance_profile(df, naive_mincuts_df, algorithms_list, objectives, xlogscales=None, xmaxvals=None, fontsize=13):\n",
    "\n",
    "    styles = [ ['blue', '--'], ['green', '-.'], ['orange', '--'], ['magenta', '-.'],  ['red', '--'], ['purple', '-.'], ['brown', '--'], ['cyan', '--'] ]\n",
    "    \n",
    "    # Create a figure with subplots side by side\n",
    "    fig, axes = plt.subplots(1, len(objectives), figsize=(16, 4))\n",
    "\n",
    "    for i, objective in enumerate(objectives):\n",
    "        # Compute the minimum objective per instance\n",
    "        min_objective_per_instance = (\n",
    "                df[df['algorithm'].isin(algorithms_list)].groupby(['hypergraph', 'seed'])\n",
    "                [objective].min()\n",
    "                .reset_index()\n",
    "                .rename(columns={objective: 'min_objective'})\n",
    "        )\n",
    "\n",
    "        # Initialize the overall max ratio (used for the x limits of the plot)\n",
    "        overall_max_ratio = 2\n",
    "        \n",
    "        for j, algorithm in enumerate(algorithms_list):\n",
    "            subset = df[df['algorithm'] == algorithm]\n",
    "     \n",
    "            # Join add the minimum objective to every instance of the subset\n",
    "            subset = pd.merge(subset, min_objective_per_instance, on=['hypergraph', 'seed'], how='inner')\n",
    "            \n",
    "            # Compute the ratio to the best for each entry\n",
    "            def compute_ratio_to_best(row):\n",
    "                if row['min_objective'] == 0:\n",
    "                    return 1.0 if row[objective] == 0 else 1000.0\n",
    "                return row[objective] / row['min_objective']\n",
    "            \n",
    "            subset['ratio_to_best'] = subset.apply(compute_ratio_to_best, axis=1)\n",
    "\n",
    "            # Update the overall max ratio (if necessary)\n",
    "            max_ratio = subset['ratio_to_best'].max(skipna=True)\n",
    "            if max_ratio > overall_max_ratio:\n",
    "                overall_max_ratio = max_ratio\n",
    "            \n",
    "            # Sort the values by the ratio\n",
    "            subset = subset.sort_values(by='ratio_to_best').reset_index()\n",
    "\n",
    "            # Compute the sorted index (floating value ranging from 0 to 1)\n",
    "            subset['ind'] = subset.index / (max(len(subset), len(naive_mincuts_df)) -1)\n",
    "   \n",
    "            # Plot the algorithm to the performance profile\n",
    "            axes[i].plot(subset['ratio_to_best'], subset['ind'], label=algorithm, linewidth=3, linestyle=styles[j][1], color=styles[j][0])    \n",
    "\n",
    "        # Set x-axis to logarithmic scale if the overall max ratio is high\n",
    "        if xlogscales and xlogscales[i]:\n",
    "            axes[i].set_xscale('log')  \n",
    "        axes[i].set_title(f'{objective} performance profile', fontsize=fontsize)\n",
    "        axes[i].set_xlabel(r'$\\tau$', fontsize=fontsize)\n",
    "        axes[i].set_xlim([0.99, xmaxvals[i] if xmaxvals and xmaxvals[i] else overall_max_ratio])\n",
    "        axes[i].set_ylim([0, 1])\n",
    "        axes[i].set_ylabel(rf'% instances $\\leq \\tau \\cdot$ best instance', fontsize=fontsize)\n",
    "        axes[i].grid(True, linestyle=':')\n",
    "\n",
    "    # Hide y-ticks for all but the first subplot\n",
    "    for ax in axes[1:]:  \n",
    "        ax.yaxis.set_tick_params(labelleft=False)\n",
    "        ax.set_ylabel('')\n",
    "\n",
    "    # Collect legend handles and labels from all subplots\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "\n",
    "    # Add legend to differentiate between the algorithms\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=len(algorithms_list), frameon=True, fontsize=fontsize)\n",
    "\n",
    "    # Adjust layout to make space for the legend\n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 1])  # Leaves space at the bottom for the legend\n",
    "\n",
    "    plt.savefig(f'./performance_plots.pdf')\n",
    "    plt.show()    # Get the list of the different parameter configurations/suffixes of the sequential version of the algorithm\n",
    "    parameter_suffix_list = [a.removeprefix(sequential_algorithm) for a in algorithms_list if a.startswith(sequential_algorithm) and \"parallel\" not in a]\n",
    "\n",
    "    if not parameter_suffix_list or len(parameter_suffix_list) == 0:\n",
    "        print(\"Could not find the sequential version of the algorithm\")\n",
    "        return\n",
    "\n",
    "    # Filter for a specific hypergraphs (if necessary)\n",
    "    if hypergraphs and len(hypergraphs) > 0:\n",
    "        df = df[df['hypergraph'].isin(hypergraphs)]\n",
    "\n",
    "    group_methods = [{\"name\": \"geometric mean\", \"func\": geometric_mean}, {\"name\": \"average\", \"func\": average}];\n",
    "\n",
    "      # Create a figure with subplots side by side\n",
    "    fig, axes = plt.subplots(len(group_methods), len(fields), figsize=(16, 8))\n",
    "\n",
    "    for i, group_method in enumerate(group_methods):\n",
    "        for j, field in enumerate(fields):\n",
    "            for param_suffix in parameter_suffix_list:\n",
    "                # Build the common name for the parallel versions of the algorithm\n",
    "                parallel_common_name = sequential_algorithm + \"_parallel\" + param_suffix + \"_T\"\n",
    "                # Get the list of threads used by the parallel version of the algorithm\n",
    "                parallel_threads_list = sorted([int(a.removeprefix(parallel_common_name)) for a in algorithms_list if a.startswith(parallel_common_name)])\n",
    "            \n",
    "                # Build the list of the geometric means / averages for the passed field starting with the sequential algorithm\n",
    "                xvals = [group_method[\"func\"](df[df['algorithm'] == (sequential_algorithm + param_suffix)][field])]\n",
    "                for t in parallel_threads_list:\n",
    "                    subset = df[df['algorithm'] == (parallel_common_name + str(t))]\n",
    "                    xvals.append(group_method[\"func\"](subset[field]))\n",
    "                    \n",
    "                axes[i, j].plot([0.5] + parallel_threads_list, xvals, label=param_suffix[1:], linewidth=3)   \n",
    "    \n",
    "            # Set plot properties\n",
    "            axes[i, j].set_xscale('log', base=2)  # Set x-axis to logarithmic scale\n",
    "            axes[i, j].set_title(f'{field} strong scalability for {sequential_algorithm}', fontsize=fontsize)\n",
    "            axes[i, j].set_ylabel(f'{field} {group_method[\"name\"]}', fontsize=fontsize)\n",
    "            axes[i, j].set_xlabel(f'#threads (log scale)', fontsize=fontsize)\n",
    "            axes[i, j].grid(True, linestyle=':')\n",
    "            # Add legend to differentiate between the algorithms\n",
    "            axes[i, j].legend()\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./strong_scalability_{sequential_algorithm}.pdf')\n",
    "    plt.show()\n",
    "\n",
    "def print_submodular_stats(algorithms_list):\n",
    "    # Define the structure of the \"all_iterations.csv\" files\n",
    "    all_iterations_columns = ['initial_num_nodes', 'num_iterations', 'num_contractions_per_it', 'hypergraph', 'seed', 'algorithm']\n",
    "    # Create plots for each algorithm\n",
    "    for algorithm in algorithms_list:    \n",
    "        # Define the path of the \"all_iterations.csv\" file\n",
    "        all_iterations_path = f'./{algorithm}/all_iterations.csv'\n",
    "\n",
    "        print(f\"#################### {algorithm} ####################\")\n",
    "        \n",
    "        if os.path.exists(all_iterations_path):\n",
    "            all_iterations_df = pd.read_csv(all_iterations_path, sep=';', header=None, names=all_iterations_columns)\n",
    "            # Convert the columns to numeric if not already\n",
    "            all_iterations_df['initial_num_nodes'] = pd.to_numeric(all_iterations_df['initial_num_nodes'], errors='coerce')\n",
    "            all_iterations_df['num_iterations'] = pd.to_numeric(all_iterations_df['num_iterations'], errors='coerce')\n",
    "            all_iterations_df['num_contractions_per_it'] = pd.to_numeric(all_iterations_df['num_contractions_per_it'], errors='coerce')\n",
    "            # Handle any NaN values introduced during type conversion\n",
    "            if all_iterations_df[['initial_num_nodes', 'num_iterations', 'num_contractions_per_it']].isna().any().any():\n",
    "                print(\"Warning: NaN values found in columns after conversion.\")\n",
    "\n",
    "            num_total_runs = len(all_iterations_df)\n",
    "            \n",
    "            relative_stats = {\n",
    "                'num_runs_finished': (all_iterations_df['num_iterations'].notna()).sum(),\n",
    "            }\n",
    "        \n",
    "            print(f\"num_total_runs: \\t\\t {num_total_runs}\")\n",
    "            print(f\"avg_contractions_per_it: \\t {average(all_iterations_df['num_contractions_per_it']):.3f}\")\n",
    "            print(f\"geo_mean_contractions_per_it: \\t {geometric_mean(all_iterations_df['num_contractions_per_it']):.3f}\")\n",
    "            for name, stat in relative_stats.items():\n",
    "                print(f\"{name}: \\t\\t {stat} \\t ({stat/num_total_runs * 100:.3f}%)\")\n",
    "        else:\n",
    "            print(f'Warning: No all_iterations.csv file found for {algorithm}.')\n",
    "    # Define the structure of the \"all_iterations.csv\" files\n",
    "    all_hypercactus_columns = ['initial_num_edges', 'initial_num_nodes', 'kernel_num_edges', 'kernel_num_nodes', 'hypercactus_num_edges', 'hypercactus_num_nodes', 'time', 'memory', 'hypergraph', 'seed', 'algorithm']\n",
    "    # Create plots for each algorithm\n",
    "    for algorithm in algorithms_list:    \n",
    "        # Define the path of the \"all_iterations.csv\" file\n",
    "        all_iterations_path = f'./{algorithm}/all_hypercactus_results.csv'\n",
    "\n",
    "        print(f\"#################### {algorithm} ####################\")\n",
    "        \n",
    "        if os.path.exists(all_iterations_path):\n",
    "            all_hypercactus_results_df = pd.read_csv(all_iterations_path, sep=';', header=None, names=all_hypercactus_columns)\n",
    "            # Convert the columns to numeric if not already\n",
    "            all_hypercactus_results_df['initial_num_edges'] = pd.to_numeric(all_hypercactus_results_df['initial_num_edges'], errors='coerce')\n",
    "            all_hypercactus_results_df['initial_num_nodes'] = pd.to_numeric(all_hypercactus_results_df['initial_num_nodes'], errors='coerce')\n",
    "            all_hypercactus_results_df['kernel_num_edges'] = pd.to_numeric(all_hypercactus_results_df['kernel_num_edges'], errors='coerce')\n",
    "            all_hypercactus_results_df['kernel_num_nodes'] = pd.to_numeric(all_hypercactus_results_df['kernel_num_nodes'], errors='coerce')\n",
    "            all_hypercactus_results_df['hypercactus_num_edges'] = pd.to_numeric(all_hypercactus_results_df['hypercactus_num_edges'], errors='coerce')\n",
    "            all_hypercactus_results_df['hypercactus_num_nodes'] = pd.to_numeric(all_hypercactus_results_df['hypercactus_num_nodes'], errors='coerce')\n",
    "            all_hypercactus_results_df['time'] = pd.to_numeric(all_hypercactus_results_df['time'], errors='coerce')\n",
    "            all_hypercactus_results_df['memory'] = pd.to_numeric(all_hypercactus_results_df['memory'], errors='coerce')\n",
    "            # Handle any NaN values introduced during type conversion\n",
    "            if all_hypercactus_results_df[['hypercactus_num_edges', 'hypercactus_num_nodes', 'time']].isna().any().any():\n",
    "                print(\"Warning: NaN values found in columns after conversion.\")\n",
    "\n",
    "            num_total_runs = len(all_hypercactus_results_df)\n",
    "            \n",
    "            relative_stats = {\n",
    "                'num_runs_finished': (all_hypercactus_results_df['time'].notna()).sum(),\n",
    "            }\n",
    "        \n",
    "            print(f\"num_total_runs: \\t\\t {num_total_runs}\")\n",
    "            for name, stat in relative_stats.items():\n",
    "                print(f\"{name}: \\t\\t {stat} \\t ({stat/num_total_runs * 100:.3f}%)\")\n",
    "            print(f\"avg_initial_num_edges: \\t\\t {average(all_hypercactus_results_df['initial_num_edges']):.3f}\")\n",
    "            print(f\"geo_mean_initial_num_edges: \\t {geometric_mean(all_hypercactus_results_df['initial_num_edges']):.3f}\")\n",
    "            print(f\"avg_initial_num_nodes: \\t\\t {average(all_hypercactus_results_df['initial_num_nodes']):.3f}\")\n",
    "            print(f\"geo_mean_initial_num_nodes: \\t {geometric_mean(all_hypercactus_results_df['initial_num_nodes']):.3f}\")\n",
    "            print()\n",
    "            print(f\"avg_kernel_num_edges: \\t\\t {average(all_hypercactus_results_df['kernel_num_edges']):.3f}\")\n",
    "            print(f\"geo_mean_kernel_num_edges: \\t {geometric_mean(all_hypercactus_results_df['kernel_num_edges']):.3f}\")\n",
    "            print(f\"avg_kernel_num_nodes: \\t\\t {average(all_hypercactus_results_df['kernel_num_nodes']):.3f}\")\n",
    "            print(f\"geo_mean_kernel_num_nodes: \\t {geometric_mean(all_hypercactus_results_df['kernel_num_nodes']):.3f}\")\n",
    "            print()\n",
    "            print(f\"avg_hypercactus_num_edges: \\t {average(all_hypercactus_results_df['hypercactus_num_edges']):.3f}\")\n",
    "            print(f\"geo_mean_hypercactus_num_edges:  {geometric_mean(all_hypercactus_results_df['hypercactus_num_edges']):.3f}\")\n",
    "            print(f\"avg_hypercactus_num_nodes: \\t {average(all_hypercactus_results_df['hypercactus_num_nodes']):.3f}\")\n",
    "            print(f\"geo_mean_hypercactus_num_nodes:  {geometric_mean(all_hypercactus_results_df['hypercactus_num_nodes']):.3f}\")\n",
    "            print()\n",
    "            print(f\"avg_time: \\t\\t\\t {average(all_hypercactus_results_df['time']):.3f}\")\n",
    "            print(f\"geo_mean_time: \\t\\t\\t {geometric_mean(all_hypercactus_results_df['time']):.3f}\")\n",
    "            print(f\"max_time: \\t\\t\\t {all_hypercactus_results_df['time'].max():.3f}\")\n",
    "            print(f\"avg_memory: \\t\\t\\t {average(all_hypercactus_results_df['memory']):.3f}\")\n",
    "            print(f\"geo_mean_memory: \\t\\t {geometric_mean(all_hypercactus_results_df['memory']):.3f}\")\n",
    "            print(f\"max_memory: \\t\\t\\t {all_hypercactus_results_df['memory'].max():.3f}\")\n",
    "        else:\n",
    "            print(f'Warning: No all_hypercactus_results.csv file found for {algorithm}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad37642a-dff7-46e7-ab10-9387ecb07835",
   "metadata": {},
   "source": [
    "## Read all_results.csv and all_naive_mincuts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ff1d2-76ba-4d88-b5df-cdcdf34c6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the structure and path of the \"all_results.csv\" files\n",
    "all_results_columns = ['mincut', 'time', 'memory', 'is_exact', 'hypergraph', 'seed', 'algorithm']\n",
    "all_results_path = './*/all_results.csv'\n",
    "# Define the structure and path of the \"all_naive_mincuts.csv\" file\n",
    "all_naive_mincuts_columns = ['naive_mincut', 'hypergraph', ]\n",
    "all_naive_mincuts_path = './all_naive_mincuts.csv'\n",
    "\n",
    "# Find all \"all_results.csv\" files recursively\n",
    "all_results_csv = glob.glob(all_results_path, recursive=True)\n",
    "\n",
    "# Initialize an empty list to hold all results\n",
    "all_results_df_list = []\n",
    "\n",
    "# Loop through each file and read it into a DataFrame\n",
    "for file in all_results_csv:\n",
    "    try:\n",
    "        df = pd.read_csv(file, sep=';', header=None, names=all_results_columns)\n",
    "        # Convert 'time' and 'memory' to numeric if not already\n",
    "        df['time'] = pd.to_numeric(df['time'], errors='coerce')\n",
    "        df['memory'] = pd.to_numeric(df['memory'], errors='coerce')\n",
    "        # Handle any NaN values introduced during type conversion\n",
    "        if df[['time', 'memory']].isna().any().any():\n",
    "            print(\"Warning: NaN values found in time or memory columns after conversion.\")\n",
    "        # Add the dataframe to the list\n",
    "        all_results_df_list.append(df)  \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file}: {e}\")\n",
    "\n",
    "# Combine all DataFrames into a single DataFrame\n",
    "all_results_df = pd.concat(all_results_df_list, ignore_index=True)\n",
    "\n",
    "all_naive_mincuts_df = None\n",
    "if os.path.exists(all_naive_mincuts_path):\n",
    "    all_naive_mincuts_df = pd.read_csv(all_naive_mincuts_path, sep=';', header=None, names=all_naive_mincuts_columns)\n",
    "    all_results_df = pd.merge(all_results_df, all_naive_mincuts_df, on='hypergraph', how='left')\n",
    "else:\n",
    "    print(f\"File does not exist: {all_naive_mincuts_path}\")\n",
    "\n",
    "# Get the list of algorithms\n",
    "algorithms_list = sorted(all_results_df['algorithm'].unique())\n",
    "\n",
    "print(\"Algorithms:\", algorithms_list)\n",
    "print()\n",
    "print(all_results_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e019f-cc45-4fd8-8e9c-2664c8ffe044",
   "metadata": {},
   "source": [
    "## Distribution by algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae87827-b92c-48e9-998d-0fa80caafb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_by_algorithm(\n",
    "    all_results_df, \n",
    "    algorithms_list, \n",
    "    [\"time\", \"memory\"], \n",
    "    lambda df: (df['mincut'] < df['naive_mincut']) & df['mincut'].notnull(),\n",
    "    lambda df: (df['mincut'] < df['naive_mincut']) & df['mincut'].notnull() & (df['mincut'] != 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba09bc0-2909-42d2-be0f-cbae8aa833a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: Setting num_rounds to None shows all rounds, i.e. all reductions\n",
    "plot_reduction_by_algorithm(algorithms_list, num_rounds=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01848d94-fba1-4a9a-bc2a-327355424ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance_profile(all_results_df, all_naive_mincuts_df, algorithms_list, ['mincut', 'time', 'memory'], [True, False, False], [None, 2, 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d3de38-9872-4cc6-86bc-5bec758480c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_strong_scalability(all_results_df,algorithms_list, 'submodular', ['time', 'memory'], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a9a22-34b3-4857-a1ae-34e03d1a71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_submodular_stats(algorithms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c53c8a4-b8ad-4f3f-8602-6ba526a66878",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hypercactus_stats(['hypercactus_generator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54537830-eef4-411c-9931-bb2fe43d73dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
